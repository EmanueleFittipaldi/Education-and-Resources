{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regressione Lineare con un Dataset reale.ipynb","private_outputs":true,"provenance":[{"file_id":"https://github.com/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_a_real_dataset.ipynb","timestamp":1631025040888}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TL5y5fY9Jy_x"},"source":["# Regressione Lineare con un Dataset reale\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JJZEgJQSjyK4"},"source":["## Il Dataset\n","Il [dataset per questo esercizio](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) è basato sui dati del censimento del 1990 dalla California. Il dataset è vecchio ma fornisce ancora una grande opportunità per imparare il machine learning."]},{"cell_type":"markdown","metadata":{"id":"tX_umRMMsa3z"},"source":["## Usa la versione giusta di TensorFlow\n","\n","Il codice seguente fa si che questo Colab usi la versione più recente di TensorFlow."]},{"cell_type":"code","metadata":{"id":"lM75uNH-sTv2"},"source":["#@title Esegui su TensorFlow 2.x\n","%tensorflow_version 2.x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xchnxAsaKKqO"},"source":["## Import di moduli necessari"]},{"cell_type":"code","metadata":{"id":"9n9_cTveKmse"},"source":["#@title Import dei moduli necessari\n","import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","# Le linee seguenti regolano la granularità del report.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = \"{:.1f}\".format"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_TaJhU4KcuY"},"source":["## Il dataset\n","\n","I Dataset sono spesso salvati o su disco o reperibili da uno specifico URL in [formato .csv](https://wikipedia.org/wiki/Comma-separated_values). \n","\n","Un .csv ben formato, contiene i nomi delle colonne nella prima riga, seguito da diverse righe di dati. Una virgola divide ogni valore in ogni riga. Per esempio, di seguito ci sono le prime cinque righe del file .csv:\n","\n","```\n","\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n","-114.310000,34.190000,15.000000,5612.000000,1283.000000,1015.000000,472.000000,1.493600,66900.000000\n","-114.470000,34.400000,19.000000,7650.000000,1901.000000,1129.000000,463.000000,1.820000,80100.000000\n","-114.560000,33.690000,17.000000,720.000000,174.000000,333.000000,117.000000,1.650900,85700.000000\n","-114.570000,33.640000,14.000000,1501.000000,337.000000,515.000000,226.000000,3.191700,73400.000000\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sSFQkzNlj-l6"},"source":["### Caricare il file .csv in un pandas DataFrame\n","\n","Questo Colab, come molti programmi di machine learning, reperiscono il file .csv e lo tengono in memoria come un DataFrame.\n","\n","Il codice seguente importa il file .csv nel pandas DataFrame e scala i valori della colonna `median_house_value`:"]},{"cell_type":"code","metadata":{"id":"JZlvdpyYKx7V"},"source":["# Importa il Dataset\n","training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n","\n","# Scala i valori nella colonna median_house_value di un fattore 1000\n","training_df[\"median_house_value\"] /= 1000.0\n","\n","# Mostra le prime 5 righe del Dataframe\n","training_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5inxx49n4U9u"},"source":["Scalare i valori della colonna `median_house_value` mette il valore di ogni casa nell'ordine delle migliaia. Lo scaling manterrà i valori di loss e di learning rate in un range più amichevole.\n","\n","Anche se scalare la colonna dei dati riferiti alle label è solitamente *non* essenziale, scalare i valori delle features in un modello con diverse feature *è* solitamente essenziale.\n"]},{"cell_type":"markdown","metadata":{"id":"yMysi6-3IAbu"},"source":["## Esamina il Dataset\n","\n","Una fetta importante di molti progetti di machine learning è fare conoscenza con i tuoi dati. L'API pandas fornisce una funzione `describe` che da in output la seguente statistica riguardo ogni colonna nel DataFrame:\n","\n","* `count`, che è il numero delle righe in quella colonna. Idealmente, `count` contiene lo stesso valore per ogni colonna.\n","\n","* `mean` e `std`, che contiene la media e la  [deviazione standard](https://www.okpedia.it/deviazione-standard-scarto-quadratico-medio) (scarto quadratico medio) dei valori in ogni colonna.\n","\n","* `min` e `max`, che contiene il valore minimo e massimo in ogni colonna.\n","\n","* `25%`, `50%`, `75%`, che contengono diversi [quantili](https://developers.google.com/machine-learning/glossary/#quantile).\n","\n"]},{"cell_type":"code","metadata":{"id":"rnUSYKw4LUuh"},"source":["# Ottieni le statistiche del Dataset\n","training_df.describe()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9pcW_Yjtoo8"},"source":["### Task 1: Identifica le anomalie nel Dataset\n","\n","Vedi delle anomalie (valori strani) nel dataset?"]},{"cell_type":"markdown","metadata":{"id":"UkKtxeak4C6c"},"source":["Il valore massimo (max) di diverse colonne sembra essere molto alto comparato ai loro quantili. Per esempio, prendiamo la colonna total_rooms_column. Dati i valori dei quantili 25%, 50%, and 75%, ci aspettavamo di ottenere un valore per max intorno a 5,000 o 10,000. Tuttavia, il valore max è 37,937. Quando vedi delle anomalie in una colonna, sii attento nell'usare quella colonna come una feature. Detto questo, anomalie in feature potenziali alcune volte riflettono anomalie nella label, che potrebbe rendere la colonna (e la fa sembrare) una feature potente. Inoltre, come vedrai dopo nel corso, potresti essere in grado di rappresentare (pre-processare) i dati grezzi in modo da rendere le colonne utili come features.\n"]},{"cell_type":"markdown","metadata":{"id":"3014ezH3C7jT"},"source":["## Definizione delle funzioni che costruiscono ed allenano un modello\n","\n","Il seguente codice definisce due funzioni:\n","* `build_model(my_learning_rate)`, che costruisce un modello inizializzato in modo randomico.\n","* `train_model(model, feature, label, epochs)`, che allena un modello dai dati (feature e label) che passiamo\n"]},{"cell_type":"code","metadata":{"id":"pedD5GhlDC-y"},"source":["#@title Definizione delle funzioni per costruire ed allenare un modello\n","def build_model(my_learning_rate):\n","  \"\"\"Create and compile a simple linear regression model.\"\"\"\n","  # Most simple tf.keras models are sequential.\n","  model = tf.keras.models.Sequential()\n","\n","  # Describe the topography of the model.\n","  # The topography of a simple linear regression model\n","  # is a single node in a single layer.\n","  model.add(tf.keras.layers.Dense(units=1, \n","                                  input_shape=(1,)))\n","\n","  # Compile the model topography into code that TensorFlow can efficiently\n","  # execute. Configure training to minimize the model's mean squared error. \n","  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n","                loss=\"mean_squared_error\",\n","                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","  return model        \n","\n","\n","def train_model(model, df, feature, label, epochs, batch_size):\n","  \"\"\"Train the model by feeding it data.\"\"\"\n","\n","  # Feed the model the feature and the label.\n","  # The model will train for the specified number of epochs. \n","  history = model.fit(x=df[feature],\n","                      y=df[label],\n","                      batch_size=batch_size,\n","                      epochs=epochs)\n","\n","  # Gather the trained model's weight and bias.\n","  trained_weight = model.get_weights()[0]\n","  trained_bias = model.get_weights()[1]\n","\n","  # The list of epochs is stored separately from the rest of history.\n","  epochs = history.epoch\n","  \n","  # Isolate the error for each epoch.\n","  hist = pd.DataFrame(history.history)\n","\n","  # To track the progression of training, we're going to take a snapshot\n","  # of the model's root mean squared error at each epoch. \n","  rmse = hist[\"root_mean_squared_error\"]\n","\n","  return trained_weight, trained_bias, epochs, rmse\n","\n","print(\"Defined the create_model and traing_model functions.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ak_TMAzGOIFq"},"source":["## Definizione delle funzioni per disegnare i grafici"]},{"cell_type":"code","metadata":{"id":"QF0BFRXTOeR3"},"source":["#@title Definizione delle funzioni per disegnare i grafici\n","def plot_the_model(trained_weight, trained_bias, feature, label):\n","  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n","\n","  # Label the axes.\n","  plt.xlabel(feature)\n","  plt.ylabel(label)\n","\n","  # Create a scatter plot from 200 random points of the dataset.\n","  random_examples = training_df.sample(n=200)\n","  plt.scatter(random_examples[feature], random_examples[label])\n","\n","  # Create a red line representing the model. The red line starts\n","  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n","  x0 = 0\n","  y0 = trained_bias\n","  x1 = 10000\n","  y1 = trained_bias + (trained_weight * x1)\n","  plt.plot([x0, x1], [y0, y1], c='r')\n","\n","  # Render the scatter plot and the red line.\n","  plt.show()\n","\n","\n","def plot_the_loss_curve(epochs, rmse):\n","  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Root Mean Squared Error\")\n","\n","  plt.plot(epochs, rmse, label=\"Loss\")\n","  plt.legend()\n","  plt.ylim([rmse.min()*0.97, rmse.max()])\n","  plt.show()  \n","\n","print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-IXYVfvM4gD"},"source":["## Chiamata delle funzioni del modello\n","Una parte importante del machine learning è determinare quali [features](https://developers.google.com/machine-learning/glossary/#feature) correlare con le [label](https://developers.google.com/machine-learning/glossary/#label). Per esempio, i modelli di predizione del valore delle case nella realtà si basa tipicamente su centinaia di features e su features sintetiche. Tuttavia, questo modello fa affidamento solo su una feature. Per adesso, Scegliamo arbitrariamente `total_rooms` come feature.\n"]},{"cell_type":"code","metadata":{"id":"nj3v5EKQFY8s","cellView":"both"},"source":["# I valori seguenti sono gli iperparametri\n","learning_rate = 0.01\n","epochs = 30\n","batch_size = 30\n","\n","# Specifica la feature e la label\n","my_feature = \"total_rooms\" # Il numero totale di stanze in uno specifico isolato di città\n","my_label=\"median_house_value\" # Il valore medio di una casa in uno specifico isolato di città.\n","\n","#Cioè, andiamo a creare un modello che predice il valore di una casa basandosi soltanto su total_rooms.\n","# Scartiamo qualsiasi versione pre-esistente del modello.\n","my_model = None\n","\n","# Invoca la funzione.\n","my_model = build_model(learning_rate)\n","weight, bias, epochs, rmse = train_model(my_model, training_df, \n","                                         my_feature, my_label,\n","                                         epochs, batch_size)\n","\n","print(\"\\nThe learned weight for your model is %.4f\" % weight)\n","print(\"The learned bias for your model is %.4f\\n\" % bias )\n","\n","plot_the_model(weight, bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Btp8zUNbYOcd"},"source":["Una certa quantità randomica entra in gioco ogni volta che alleni il modello. Conseguentemente, si avranno risultati differenti ogni volta. Detto questo, considerando il dataset e gli iperparametri, il modello farà generalmente un cattivo lavoro nel descrivere la relazione tra la feature selezionata e la label."]},{"cell_type":"markdown","metadata":{"id":"1xNqWWos_zyk"},"source":["## Usa il modello per fare delle predizioni\n","\n","Possiamo usare il modello allenato per fare delle predizioni. In pratica [si dovrebbero fare delle predizioni su dei dati che non sono stati usati in fase di allenamento](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). Tuttavia, per questo esercizio, lavoreremo con un sottoinsieme dello stesso dataset di training. Nelle lezioni successive esploreremo dei modi per fare delle predizioni su dati non usati in fase di allenamento.\n","\n","Prima di tutto, eseguiamo il codice per definire la funzione di predizione per le case:\n"]},{"cell_type":"code","metadata":{"id":"nH63BmncAcab"},"source":["def predict_house_values(n, feature, label):\n","  \"\"\"Predict house values based on a feature.\"\"\"\n","\n","  batch = training_df[feature][10000:10000 + n]\n","  predicted_values = my_model.predict_on_batch(x=batch)\n","\n","  print(\"feature   label          predicted\")\n","  print(\"  value   value          value\")\n","  print(\"          in thousand$   in thousand$\")\n","  print(\"--------------------------------------\")\n","  for i in range(n):\n","    print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000 + i],\n","                                   training_df[label][10000 + i],\n","                                   predicted_values[i][0] ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbBNQujU5WjK"},"source":["Adesso, invoca la funzione di predizinoe su 10 dati:"]},{"cell_type":"code","metadata":{"id":"Y_0DGBt0Kz_N"},"source":["predict_house_values(10, my_feature, my_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gGaqArcpqY3"},"source":["### Task 2: Giudica il potere predittivo del modello\n","\n","Guarda alla tabella precedente. Quano vicino è il valore predetto al valore della label? In altre parole, il nostro modello predice accuratamente il valore reale delle case?"]},{"cell_type":"code","metadata":{"id":"yVpjhUFm9uID"},"source":["#@title Risposta.\n","#La maggior parte dei valori predetti differisce significativamente\n","#dal valore delle label, quindi il modello allenato probabilmente\n","#non ha molto potere predittivo. Tuttavia, i primi 10 dati potrebbero\n","#essere non rappresentativi rispetto l'intero dataset\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLoqis3IUPSd"},"source":["## Task 3: Prova una feature diversa\n","La feature `total_rooms` ha poco potere predittivo. Una feature differente può portare ad un maggiore potere predittivo? Prova ad usare `population` come feature invece di `total_rooms`.\n","\n","Nota: Quando cambi la feature, potresti avere bisogno di cambiare anche gli iperparametri.\n"]},{"cell_type":"code","metadata":{"id":"H0ab6HD4ZO75"},"source":["my_feature = \"population\"   # Rimpiazza il ? con population o possibilmente una colonna differente\n","                   \n","\n","# Sperimenta con gli iperparametri.\n","learning_rate = 0.07\n","epochs = 100\n","batch_size = 70\n","\n","# Non cambiare niente dopo questa linea.\n","my_model = build_model(learning_rate)\n","weight, bias, epochs, rmse = train_model(my_model, training_df, \n","                                         my_feature, my_label,\n","                                         epochs, batch_size)\n","plot_the_model(weight, bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)\n","\n","predict_house_values(15, my_feature, my_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"107mDkW7U6mg"},"source":["#@title Double-click to view a possible solution.\n","\n","my_feature = \"population\" # Pick a feature other than \"total_rooms\"\n","\n","# Possibly, experiment with the hyperparameters.\n","learning_rate = 0.05\n","epochs = 18\n","batch_size = 3\n","\n","# Don't change anything below.\n","my_model = build_model(learning_rate)\n","weight, bias, epochs, rmse = train_model(my_model, training_df, \n","                                         my_feature, my_label,\n","                                         epochs, batch_size)\n","\n","plot_the_model(weight, bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)\n","\n","predict_house_values(10, my_feature, my_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nd_rHJ59AUtk"},"source":["La feature `population` produce un risultato migliore rispetto `total_rooms`?"]},{"cell_type":"code","metadata":{"id":"F0tPEtzcC-vK"},"source":["#@title Risposta.\n","\n","#L'allenamento non è interamente deterministico, ma la feature population tipicamente \n","#converge ad un valore di Root Mean Square Error leggermente maggiore rispetto\n","#la feature total_rroms. Quindi, population appare essere allo stesso livello\n","#se non peggio di total_rooms nel fare le predizioni.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8uYpyGacsIg"},"source":["## Task 4: Definisci una feature sintetica\n","\n","Abbiamo scoperto che sia `total_rooms` che `population` non risultano essere delle feature utili. Cioè, nè il numero totale delle stanze nel quartiere e nemmeno la popolazione nel quartiere sono state utili a predire il prezzo in media di una casa in quel quartiere. Forse però, il *rapporto* di `total_rooms` su `popolazione` potrebbe avere del potere predittivo. Cioè forse la densità in un isolato ha una qualche relazione con il valore di una casa.\n","\n","Per esplorare questa ipotesi fai le seguente cose:\n","\n","1. Crea una [feature sintetica](https://developers.google.com/machine-learning/glossary/#synthetic_feature) che è il rapporto tra `total_rooms` e `population`.\n","\n","2. Aggiusta gli iperparametri.\n","\n","3. Determina se questa feature sintetica produce dei valori di loss più bassi di qualsiasi altra singola feature che abbiamo provato in precedenza in questo esercizio.\n"]},{"cell_type":"code","metadata":{"id":"4Kx2xHSgdcpg"},"source":["# Definisci una feature sintetica chiamata riins_per_person\n","training_df[\"rooms_per_person\"] = training_df[\"total_rooms\"]/training_df[\"population\"] # Scrivi qui il tuo codice.\n","\n","# Non cambiare la prossima riga\n","my_feature = \"rooms_per_person\"\n","\n","# Assegna dei valori agli iperparametri\n","learning_rate = 0.5\n","epochs = 10\n","batch_size = 90\n","\n","# Non cambiare niente sotto questa riga.\n","my_model = build_model(learning_rate)\n","weight, bias, epochs, rmse = train_model(my_model, training_df,\n","                                         my_feature, my_label,\n","                                         epochs, batch_size)\n","\n","plot_the_loss_curve(epochs, rmse)\n","predict_house_values(15, my_feature, my_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRfxp_3yofe3"},"source":["#@title Double-click to view a possible solution to Task 4.\n","\n","# Define a synthetic feature\n","training_df[\"rooms_per_person\"] = training_df[\"total_rooms\"] / training_df[\"population\"]\n","my_feature = \"rooms_per_person\"\n","\n","# Tune the hyperparameters.\n","learning_rate = 0.06\n","epochs = 24\n","batch_size = 30\n","\n","# Don't change anything below this line.\n","my_model = build_model(learning_rate)\n","weight, bias, epochs, mae = train_model(my_model, training_df,\n","                                        my_feature, my_label,\n","                                        epochs, batch_size)\n","\n","plot_the_loss_curve(epochs, mae)\n","predict_house_values(15, my_feature, my_label)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBiDWursB1Wi"},"source":["Basandoci sui valori di loss, questa feature sintetica produce un modello migliore rispetto le singole feature che abbiamo provato nel Task 2  enel Task 3. Tuttavia, il modello non crea ancora delle predizioni buone.\n"]},{"cell_type":"markdown","metadata":{"id":"XEG_9oU9O54u"},"source":["## Task 5. Trova la feature (le features) i cui valori grezzi sono correlati con la label\n","\n","Sino ad adesso, ci siamo basati su un approccio trial-error per identificare le possibili features per il modello. E' più saggio invece fare affidamento sulla statistica.\n","\n","Una **matrice di correlazione** indica come ogni valore degli attributi grezzi sono correlati ai valori di altri attributi grezzi. I valori di correlazione hanno i seguenti significati:\n","\n","* `1.0`: correlazione perfettamente positiva; ovvero, quando un attributo cresce, anche l'altro attributo cresce.\n","\n","*`-1.0`: correlazione perfettamente negativa; ovvero, quando un attributo cresce, l'altro decresce.\n","\n","*`0.0`: nessuna correlazione; le due colonne non sono  [linearmente dipendenti](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg).\n","\n","In generale, più è alto il valore della correlazione, più è grande il potere predittivo. Per esempio, un valore di correlazione di -0.8 implica un potere predittivo di gran lunga maggiore di una correlazione di -0.2 (si guarda in sostanza al valore assoluto della correlazione per giudicare il suo potere).\n","\n","Il seguente codice genera la matrice di correlazione per gli attributi del Dataset delle case in california:\n"]},{"cell_type":"code","metadata":{"id":"zFGKL45LO8Tt"},"source":["# Genere una matrice di correlazione.\n","training_df.corr()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hp0r3NAVPEdt"},"source":["La matrice di correlazione mostra nove potenziali features (inclusa una sintetica) ed una label (`median_house_value`). Una correlazione fortemente negativa od una estremamente positiva con la label suggeriscono una geature potenzialmente molto buona.\n","\n","**il tuo compito**: Determinare quali delle seguenti features potenziali appaiono essere le migliori candidate per essere assunte come features per il modello.\n"]},{"cell_type":"code","metadata":{"id":"RomQTd1OPVd0"},"source":["#@title Risposta\n","\n","#La feature 'median_income' si correla con lo 0.7 alla label median_house_value,\n","#quindi 'median_income potrebbe essere una buona feature'. Le altri sette features \n","#potenziali hanno tutte una correlazione vicino allo 0.\n","\n","#Se il tempo permette, prova la feature median_income e vedi come il modello migliora\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8RqvEbaVSlRt"},"source":["Le matrici di correlazione non ci dicono tutta la storia. Nei seguenti esercizi, troveremo dei modi aggiuntivi per sbloccare il potere predittivo dalle feature potenziali.\n"]}]}