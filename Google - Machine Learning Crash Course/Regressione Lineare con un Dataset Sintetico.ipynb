{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regressione Lineare con un Dataset Sintetico.ipynb","private_outputs":true,"provenance":[{"file_id":"https://github.com/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb","timestamp":1631020595843}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TL5y5fY9Jy_x"},"source":["# Semplice Linear Regression con dei Dati Sintetici"]},{"cell_type":"markdown","metadata":{"id":"bMr7MPVmoiHf"},"source":["## Usa la versione giusta di TensorFlow\n","\n","La seguente riga di codice assicura che Colab eseguirà TensorFlow 2.X, corrispondente alla versione più recente di TensorFlow:"]},{"cell_type":"code","metadata":{"id":"Z1pOWL7eevO8"},"source":["#@title Run this Colab on TensorFlow 2.x\n","%tensorflow_version 2.x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xchnxAsaKKqO"},"source":["## Import di moduli necessari\n","\n"]},{"cell_type":"code","metadata":{"id":"9n9_cTveKmse"},"source":["import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIpsyJITPcbG"},"source":["#Definizione delle funzioni che costruiscono e allenano un modello\n","\n","Il seguente codice definisce due funzioni:\n","\n","* `build_model(my_learning_rate)`, che costruisce un modello vuoto\n","* `train_model(model, feature, label, epochs)`, che allena il modello a partire dai dati (feature e label) che gli passi.\n","\n","Dato che non è necessario per adesso capire come il codice per costruire il modello è stato sviluppato, lo abbiamo nascosto. Possiamo però fare doppio click per esplorarlo.\n"]},{"cell_type":"code","metadata":{"id":"xvO_beKVP1Ke"},"source":["#@title Definizione delle funzioni che costruiscono ed allenano il modello\n","\n","def build_model(my_learning_rate):\n","  \"\"\"Crea e compila un semplice modello di regressione lineare.\"\"\"\n","  # I modelli più semplici di tf.keras sono sequenziali.\n","  # Un modello sequenziale contiene uno o più livelli.\n","  model = tf.keras.models.Sequential()\n","\n","  # Descrive la topografia del modello.\n","  # La topografia di un semplice modello di regressione lineare\n","  # è un singolo nodo in un singolo livello\n","  model.add(tf.keras.layers.Dense(units=1, \n","                                  input_shape=(1,)))\n","\n","  # Compila la topografia del modello in codice che TensorFlow\n","  # può eseguire efficientemente. Configura l'allenamento per\n","  # minimizzare l'errore quadratico medio del modello\n","  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n","                loss=\"mean_squared_error\",\n","                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","  return model           \n","\n","\n","def train_model(model, feature, label, epochs, batch_size):\n","  \"\"\"Allena il modello dandogli in pasto i dati.\"\"\"\n","\n","  # Dai in pasto al modello i valori delle features e i valori delle lables.\n","  # Il modello si allenerà per il numero specificato di epoche, imparando\n","  # gradualmente come i valori delle features sono in relazione con\n","  # i valori delle lables.\n","  history = model.fit(x=feature,\n","                      y=label,\n","                      batch_size=batch_size,\n","                      epochs=epochs)\n","\n","  # Raccoglie il weight e bias del modello allenato\n","  trained_weight = model.get_weights()[0]\n","  trained_bias = model.get_weights()[1]\n","\n","  # La lista delle epoche è salvata separatamente rispetto \n","  # il resto della history.\n","  epochs = history.epoch\n","  \n","  # Raccoglie la history (una snapshot) di ogni epoca.\n","  hist = pd.DataFrame(history.history)\n","\n","  # Raccoglie specificamente l'errore quadratico medio \n","  # del modello ad ogni epoca.\n","  rmse = hist[\"root_mean_squared_error\"]\n","\n","  return trained_weight, trained_bias, epochs, rmse\n","\n","print(\"Defined create_model and train_model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ak_TMAzGOIFq"},"source":["## Definizione delle funzioni per il plotting\n","\n","Usiamo una libreria molto conosciuta chiamata  [Matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) per creare i seguenti due grafici:\n","\n","* Un grafico dei valori delle features vs. i valori delle lables, ed una linea che mostra l'output del modello allenato.\n","* Una [curva di loss](https://developers.google.com/machine-learning/glossary/#loss_curve).\n"]},{"cell_type":"code","metadata":{"id":"QF0BFRXTOeR3"},"source":["#@title Definizione delle funzioni di plotting\n","def plot_the_model(trained_weight, trained_bias, feature, label):\n","  \"\"\"Disegna il modello allenato rispetto feature e le label usate per il training.\"\"\"\n","\n","  # Diamo un nome agli assi.\n","  plt.xlabel(\"feature\")\n","  plt.ylabel(\"label\")\n","\n","  # Disegna i valori delle features vs. i valori delle lables\n","  plt.scatter(feature, label)\n","\n","  # Crea una linea rossa rappresentante il modello. La linea rossa\n","  # ha origine alle coordinate (x0, y0) e termina alle coordinate (x1, y1).\n","  x0 = 0\n","  y0 = trained_bias\n","  x1 = feature[-1]\n","  y1 = trained_bias + (trained_weight * x1)\n","  plt.plot([x0, x1], [y0, y1], c='r')\n","\n","  # Renderizza il grafico a dispersione e la linea rossa\n","  plt.show()\n","\n","def plot_the_loss_curve(epochs, rmse):\n","  \"\"\"Disegna la curva di loss, che mostra la loss vs. le epoche.\"\"\"\n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Root Mean Squared Error\")\n","\n","  plt.plot(epochs, rmse, label=\"Loss\")\n","  plt.legend()\n","  plt.ylim([rmse.min()*0.97, rmse.max()])\n","  plt.show()\n","\n","print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVSDPusELEZ5"},"source":["\n","## Definizione deld ataset\n","\n","Il dataset è costituito da 12 dati.\n"]},{"cell_type":"code","metadata":{"id":"rnUSYKw4LUuh"},"source":["my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n","my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K24afla-4s2x"},"source":["## Specifica gli iperparametri\n","\n","Gli iperparametri in questo Colab sono:\n","\n","  * [learning rate](https://developers.google.com/machine-learning/glossary/#learning_rate)\n","  * [epoche](https://developers.google.com/machine-learning/glossary/#epoch)\n","  * [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size)\n","\n","Il codice seguente inizializza questi iperparametri e poi invoca le funzioni per costruire ed allenare il modello."]},{"cell_type":"code","metadata":{"id":"Ye730h13CQ97"},"source":["learning_rate=0.01\n","epochs=10\n","my_batch_size=12\n","\n","my_model = build_model(learning_rate)\n","trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n","                                                         my_label, epochs,\n","                                                         my_batch_size)\n","plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwSm60H6pQjJ"},"source":["## Task 1: Esamina il grafico\n","\n","Esamina il grafo sovrastante. I punti blu rappresentano i dati; la linea rossa rappresenta l'outpu del modello allenato. Idealmente, la linea rossa dovrebbe allinearsi armoniosamente con i punti blu. Lo fa? No.\n","\n","Esamina il grafo sottostante, che mostra la curva di loss. Nota come la curva di loss decresce ma non si appiattisce, che è un segno che il modello non si è allenato a sufficienza.\n","\n","Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently."]},{"cell_type":"markdown","metadata":{"id":"lLXPvqCRvgI4"},"source":["## Task 2: Aumenta il numero delle epoche\n","\n","La loss dovrebbe decrementare in modo costante, dapprima bruscamente, e poi più lentamente. Eventualmente, la loss dovrebbe rimanere stabile (con pendenza zero o quasi zero), che indica che l'allenamento ha raggiunto la [convergenza](http://developers.google.com/machine-learning/glossary/#convergence).\n","\n","In Task 1, la loss non ha raggiunto la convergenza. Una possibile soluzione è allenare il modello per più epoche. Il tuo task è aumentare il numero delle epoche sufficientemente per far si che il modello converga. Tuttavia, è inefficiente allenare il modello oltrepassata la convergenza, quindi non impostare semplicemente un numero di epoche arbitrariamente molto alto.\n","\n","Esamina la curva di loss. Il modello converge?\n"]},{"cell_type":"code","metadata":{"id":"uXuJH3h6t5qs"},"source":["learning_rate=0.01\n","epochs= 450  # Replace ? with an integer.\n","my_batch_size=12\n","\n","my_model = build_model(learning_rate)\n","trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n","                                                        my_label, epochs,\n","                                                        my_batch_size)\n","plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KmzfFB5zwvd"},"source":["## Task 3: Aumenta il Learning rate\n","\n","Nel Task 2, abbiamo incrementato i l numero di epoche per far si che il modello raggiungesse la convergenza. A volte, possiamo raggiungere la convergenza molto più velocemente aumentando il learning rate. Tuttavia, impostando un learning rate troppo altro, spesso rende il modello impossibilitato a convergere. Nel Task 3, abbiamo intenzionalmente impostato il learning rate troppo alto.\n"]},{"cell_type":"code","metadata":{"id":"eD1hTmdd0uCo"},"source":["# Aumenta il learning rate e diminuisci il numero delle epoche.\n","learning_rate=100  \n","epochs=500  \n","\n","# Iparametri perfetti invece sono\n","# learning_rate=0.14\n","# epochs=70\n","\n","my_model = build_model(learning_rate)\n","trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n","                                                         my_label, epochs,\n","                                                         my_batch_size)\n","plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c96ITm021NEV"},"source":["Il modello risultante è terribile; la linea rossa non si allinea con i punti blu. Inoltre, la curva di loss oscilla come una [montagna russa](https://www.wikipedia.org/wiki/Roller_coaster). Una curva di loss che oscilla suggerisce fortemente che il learning rate è troppo alto."]},{"cell_type":"markdown","metadata":{"id":"0NDET9e6AAbA"},"source":["## Task 5: Regola la batch size\n","\n","Il sistema ricalcola il valore di loss del modello regolando i pesi e dell'intercetta dopo ogni **iterazione**. Ogni iterazione è il frangente in cui il sistema processa una batch. Per esempio, se la **batch size** è 6, allora il sistema ricalcola il valore della loss del modello regolando i pesi del modello e l'intercetta dopo ogni 6 dati.\n","\n","Una **epoca** è il frangente sufficiente di ogni iterazione per processare ogni dato presente nel dataset. Per esempio, se la batch size è 12, allora ogni epoca dura una iterazione. Tuttavia, se la batch size è 6, allora ogni epoca consuma due iterazioni.\n","\n","Si potrebbe essere tentati dall'impostare la batch size uguale al numero dei dati nel dataset (12, in questo caso). Tuttavia, il modello potrebbe allenarsi più velocemente su batch più piccole. Al contrario, su delle batch size molto piccole, potrebbe non contenere abbastanza informazioni per aiutare il modello a convergere.\n","\n","Esperimenta con la `batch_size`nel seguente codice. Quale è il più piccolo intero che è possibile dare a `batch_size` ed avere ancora il modello che raggiunge la convergenza in un centinaio di epoche? \n"]},{"cell_type":"code","metadata":{"id":"_vGx0lOodQrT"},"source":["learning_rate=0.05\n","epochs=125\n","my_batch_size= 1  # Replace ? with an integer. Settando 1 praticamente ho realizzato la Stocastic Gradient Descent la quale usa solo 1 dato.\n","\n","my_model = build_model(learning_rate)\n","trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n","                                                        my_label, epochs,\n","                                                        my_batch_size)\n","plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n","plot_the_loss_curve(epochs, rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aS3q7TIF9SFL"},"source":["## Ricapitolo della calibrazione degli iperparametri\n","\n","La maggior parte dei problemi di machine learning richiedono molto tuning degli iperparametri. Sfortunatamente, non possiamo fornire delle regole concrete per la calibrazione di ogni modello. Abbassare il learning rate può aiutare un modello a convergere efficientemente ma potrebbe rendere un altro modello troppo lento a raggiungere la convergenza. Devi sperimentare per trovare il miglior insieme di iperparametri per il tuo dataset. Detto questo, ecco delle linee guida:\n","\n","* La loss dovrebbe decrescere in modo costante, dapprima bruscamente, e poi man mano più gradualmente finchè la pendenza della curva raggiunge o si avvicina allo zero.\n","* Se la loss non converge, allena il modello per più epoche.\n","* Se la loss decrementa troppo lentamente, aumenta il learning rate. Nota che impostando il learning rate troppo alto potrebbe prevenire la loss dal convergere.\n","* Se la loss varia in modo bizzarro (cioè, la loss salta in giro) decrementare il learning rate.\n","* Diminuire il learning rate mentre si incrementa il numero di epoche o la batch size è spesso una buona combinazione.\n","* Impostare la batch size ad un numero *molto* basso potrebbe anche causare instabilità. Per prima cosa, prova un valore per la batch size grande. Poi, decrementalo finchè non vedi un peggioramento. \n","\n","* Per i dataset del mondo-reale i quali contengono milioni o miliardi di dati, l'intero dataset potrebbe addirittura non entrare in memoria. In questi casi, avrai bisogno di ridurre la batch size per permettere alla batch di entrare in memoria.\n","\n","Ricorda: La combinazione ideale di iperparametri è dipendente dai dati, quindi devi sempre sperimentare e verificare.\n"]}]}