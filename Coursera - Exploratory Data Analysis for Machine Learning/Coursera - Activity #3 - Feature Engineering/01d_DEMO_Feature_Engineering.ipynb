{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"01d_DEMO_Feature_Engineering.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"5fDz4-CJkur7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"611XnjGBhAMz"},"source":["# Machine Learning Foundation\n","\n","## Section 1, Part d: Feature Engineering "]},{"cell_type":"markdown","metadata":{"id":"wFmWqN3yhAM2"},"source":["## Feature Engineering with Linear Regression: Applied to the Ames Housing Data\n","\n","Using the Ames Housing Data:\n","\n","Dean De Cock\n","Truman State University\n","Journal of Statistics Education Volume 19, Number 3(2011), www.amstat.org/publications/jse/v19n3/decock.pdf\n","\n","In this notebook, we will build some linear regression models to predict housing prices from this data. In particular, we will set out to improve on a baseline set of features via **feature engineering**: deriving new features from our existing data. Feature engineering often makes the difference between a weak model and a strong one.\n","\n","We will use visual exploration, domain understanding, and intuition to construct new features that will be useful later in the course as we turn to prediction.\n","\n","**Notebook Contents**\n","\n","> 1. Simple EDA \n","> 2. One-hot Encoding variables\n","> 3. Log transformation for skewed variables\n","> 4. Pair plot for features\n","> 5. Basic feature engineering: adding polynomial and interaction terms\n","> 6. Feature engineering: categories and features derived from category aggregates \n","\n","## 1. Simple EDA "]},{"cell_type":"code","metadata":{"id":"CFRtcYFXhAM3"},"source":["%pylab inline\n","%config InlineBackend.figure_formats = ['retina']\n","\n","import pandas as pd\n","import seaborn as sns\n","sns.set() # stiamo impostando uno stile standard per l'output dei grafici di seaborn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3vzeWY3hAM4"},"source":["#### Load the Data, Examine and Explore"]},{"cell_type":"code","metadata":{"id":"k2H4zkpEhAM4"},"source":["## Load in the Ames Housing Data\n","datafile = \"/content/drive/MyDrive/Colab Notebooks/Coursera - Exploratory Data Analysis for Machine Learning/Coursera - Activity #3 - Feature Engineering/data/Ames_Housing_Data.tsv\"\n","df = pd.read_csv(datafile, sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwaNXI3Xopz0"},"source":["# Voglio dare uno sguardo alle primo 5 righe\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0t74d4mhAM5"},"source":["# la funzione info() di pandas mostra un riassunto conciso del DataFrame.\n","# mostra informazioni riguardo un DataFrame incluso:\n","# 1) il type dell'indice e delle colonne\n","# 2) Il nome di tutte le colonne\n","# 3) Quanti valori non null ci sono per ogni colonna \n","# 4) La quantità di memoria in uso\n","\n","## Examine the columns, look at missing data\n","df.info()\n","\n","# Come possiamo vedere, per esempio \"Alley\" ha pochi valori non nulli, solo 198. Sono molto pochi \n","# Se consideriamo che abbiamo 2930 dati."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X-BnEQ6rxHg"},"source":["df['Gr Liv Area'].hist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zv1BuaGJi7HD"},"source":["df.loc[df['Gr Liv Area']<=4000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qmqobTthAM5"},"source":["# Come possiamo osservare dall'istogramma qui sopra, non ci sono molti valori sopra 4000\n","# Probabilmente ci sono dei valori anomali che possiamo scartare.\n","\n","# This is recommended by the data set author to remove a few outliers\n","\n","df = df.loc[df['Gr Liv Area'] <= 4000,:]\n","print(\"Number of rows in the data:\", df.shape[0])\n","print(\"Number of columns in the data:\", df.shape[1])\n","data = df.copy() # Keep a copy our original data because it is a best practice"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbDTuh6qhAM6"},"source":["# A quick look at the data:\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjt6FYQytCQq"},"source":["# Se controlliamo quanti valori unici assume PID notiamo che sono esattamente 2925, ovvero tanti\n","# quanti sono i dati presenti nel data set. Questo significa che ogni dato ha un PID unico e questo\n","# non ci serve a niente in quanto non ha un contenuto informativo utile per un algoritmo di ML.\n","# Lo stesso discorso vale per Order. Quindi facciamo un drop di queste due colonne\n","len(df.PID.unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruuqX1W1tkz-"},"source":["df.drop(['PID','Order'],axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVRfL1GPhAM6"},"source":["We're going to first do some basic data cleaning on this data: \n","\n","* Converting categorical variables to dummies\n","* Making skew variables symmetric\n","\n","### One-hot encoding for dummy variables:"]},{"cell_type":"code","metadata":{"id":"qP9Ri9oMhAM7"},"source":["# Get a Pd.Series consisting of all the string categoricals\n","one_hot_encode_cols = df.dtypes[df.dtypes == np.object]  # filtering by string categoricals\n","one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n","\n","df[one_hot_encode_cols].head().T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VykSuqIbhAM8"},"source":["We're going to first do some basic data cleaning on this data: \n","\n","* Converting categorical variables to dummies\n","* Making skew variables symmetric\n","\n","#### One-hot encoding the dummy variables:"]},{"cell_type":"code","metadata":{"id":"c-VGgd3KhAM8"},"source":["# Do the one hot encoding\n","df = pd.get_dummies(df, columns=one_hot_encode_cols, drop_first=True)\n","df.describe().T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4TSABc0hAM8"},"source":["### Log transforming skew variables\n","\n","Per permettere ai dati di non avere un bias ma bensì di essere normalmente distribuiti, applichiamo una Log transformation ai valori numerici. "]},{"cell_type":"code","metadata":{"id":"UHJQh17Z0LM4"},"source":["# Esempio di come filtrare soltanto alcune colonne in base al tipo di valori contenuti in esse.\n","# Questa funzione filtra le colonne mostrando solo le colonne contenenti valori numerici.\n","df.select_dtypes(\"number\") # similmente per object, int\n","                           # aggiungendo .columns ritorna tutti i nomi delle colonne che contengono valori numerici\n","df.select_dtypes(\"number\").columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrabdbhrhAM9"},"source":["# A differenza dell'esemmpio di prima, voglio prendere soltanto le colonne che contengono valori float.\n","\n","# Create a list of float colums to check for skewing\n","mask = data.dtypes == np.float      # è una maschera dove ci sono tutti i nomi delle colonne e True se la colonna contiene valori float\n","float_cols = data.columns[mask]     # ritorna i nomi delle colonne contenenti valori con la virgola\n","skew_limit = 0.75                   # define a limit above which we will log transform\n","\n","# valori numerici maggiori di 0 significa che i dati hanno una distorsione positiva, ovvero una pendenza verso sinistra, mentre\n","# valori numerici minori di 0 stanno ad indicare che i dati hanno una distorsione negativa, ovvero una pendenza verso destra.\n","# settendo lo skew_limit stiamo imponendo la tolleranza massima che abbiamo nei confronti \n","# dei dati che pendono verso un lato, oltre la quale applichiamo la log transformation\n","\n","\n","# questa funzione mi mostra quanto distorti sono i miei dati\n","skew_vals = data[float_cols].skew() # applico il treshold per lo skew alle sole colonne contenenti dati numerici"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAhDRcol53C0"},"source":["# Diamo un occhiata a quanto distorti sono i dati\n","skew_vals # da come si evince, Mas Vnr Area, Lot Frontage, MsmtFin SF 2, Bsmt Full Bath e Bsmt Halft Bath hanno una forte\n","# pendenza verso destra mentre Garage Yr Blt, Garage cars hanno una pendenza a sinistra."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VX3xFksuhAM9"},"source":["# Filtro il risultato precedente in modo da mostrare soltanto le colonne che \n","# superano lo skew_limit impostato\n","skew_cols = (skew_vals\n","             .sort_values(ascending=False)\n","             .to_frame()\n","             .rename(columns={0:'Skew'})\n","             .query('abs(Skew) > {}'.format(skew_limit)))\n","\n","skew_cols"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTL45_gthAM9"},"source":["# Let's look at what happens to one of these features, when we apply np.log1p visually.\n","\n","# Choose a field\n","field = \"Mas Vnr Area\"\n","\n","# Usiamo plt.subplots perché così possiamo mostrare due grafici nella stessa figura.\n","# Create two \"subplots\" and a \"figure\" using matplotlib\n","fig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(10, 5))\n","\n","# Create a histogram on the \"ax_before\" subplot\n","df[field].hist(ax=ax_before)\n","\n","# Apply a log transformation (numpy syntax) to this column\n","df[field].apply(np.log1p).hist(ax=ax_after)\n","\n","\n","# Formatting of titles etc. for each subplot\n","ax_before.set(title='before np.log1p', ylabel='frequency', xlabel=\"Inital Skew: \" +str( df[field].skew() ))\n","ax_after.set(title='after np.log1p', ylabel='frequency', xlabel=\"After Skew: \" +str( df[field].apply(np.log1p).skew()))\n","fig.suptitle('Field \"{}\"'.format(field));"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9f8hNtZhAM-"},"source":["# Quanto visto sopra è per far vedere come applicando una Log Transformation\n","# la distribuzione dei valori cambia, distribuendo i dati in modo più normale.\n","# Adesso dobbiamo effettuare la trasformazione per tutti i valori contenute in tutte\n","# quelle colonne che superano lo skew limit definito in precedenza. Da notare che\n","# non vogliamo applicare questa trasformazione anche a SalePrice per la quale colonna\n","# skipperemo.\n","\n","# Perform the skew transformation:\n","\n","for col in skew_cols.index.values:\n","    if col == \"SalePrice\":\n","        continue\n","    df[col] = df[col].apply(np.log1p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pf1-wuUfhAM-"},"source":["# We now have a larger set of potentially-useful features\n","df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdAXC61ehAM-"},"source":["# There are a *lot* of variables. Let's go back to our saved original data and look at how many values are missing for each variable. \n","df = data\n","data.isnull().sum().sort_values() # isnull ritorna true o false se il dato è null oppure no. Sum conta quanti ce ne sono in totale in quella colonna\n","                                  # mentre sort_values ordina queste sommatorie dalla più piccola alla più grande\n","\n","# Osservando il risultato, la colonna con più valore NULL è \"Pool QC\", mentre al secondo e terzo posto troviamo \"Misc Feature\" e \"Alley\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sir34niwhAM-"},"source":[" Let's pick out just a few numeric columns to illustrate basic feature transformations."]},{"cell_type":"code","metadata":{"id":"u4rrv0JHhAM-"},"source":["smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n","                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n","                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n","                      'Garage Cars','SalePrice']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTdgXPZbhAM_"},"source":["# Now we can look at summary statistics of the subset data\n","smaller_df.describe().T # con .T facciamo la trasposta del DataFrame in modo tale che le statistiche stanno per colonne "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvFKYMlkhAM_"},"source":["smaller_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oaBd56ChAM_"},"source":["# There appears to be one NA in Garage Cars - we will take a simple approach and fill it with 0\n","# altri approcci potevano essere quelli di sostituire quel valore null con per esempio la mediana o la media\n","smaller_df = smaller_df.fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwaE5HrXhAM_"},"source":["smaller_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2ybsuU1hANA"},"source":["\n","### Pair plot of features\n","Now that we have a nice, filtered dataset, let's generate visuals to better understand the target and feature-target relationships: pairplot is great for this!"]},{"cell_type":"code","metadata":{"id":"RHaTQ9G2hANA"},"source":["sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7w_VqA6ahANA"},"source":["---\n","**Data Exploration Discussion**: \n","\n","1. What do these plots tell us about the distribution of the target? il target qui è sales price. Guardando la sua distribuzione, ovvero l'ultimo riquadro in basso a destra osserviamo che SalePrice ha una distorsione positiva, ma questo ce lo aspettavamo in quanto prima avevamo escluso questa colonna dalla correzione dello skew.   \n","\n","2. What do these plots tell us about the relationship between the features and the target? Do you think that linear regression is well-suited to this problem? Do any feature transformations come to mind? Se osserviamo la relazione tra SalePrice e Overall Qual, notiamo che c'è una leggera curva. Potrebbe essere quindi una buona idea introdurre (Overall Qual)^2 in modo da riuscire a rappresentare questo andamento parabolico. Lo stesso discorso vale per Gr Liv Area, il cui andamento dei dati in rapporto a SalePrice sembra essere parabolico. Per questo motivo introduciamo anche (Gr Liv Area)^2\n","\n","3. What do these plots tell us about the relationship between various pairs of features? Do you think there may be any problems here?  Non ci sono evidenti problemi di colinearità\n","\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"pbBAtF6ShANA"},"source":["#### Suppose our target variable is the SalePrice. We can set up separate variables for features and target."]},{"cell_type":"code","metadata":{"id":"pQPNF3A1hANA"},"source":["#Separate our features from our target\n","\n","X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n","                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n","                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n","                      'Garage Cars']]\n","\n","y = smaller_df['SalePrice']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gLwSlZnlhANB"},"source":["X.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7GrhuK7zhANB"},"source":["Now that we have feature/target data X, y ready to go, we're nearly ready to fit and evaluate a baseline model using our current feature set. We'll need to create a **train/validation split** before we fit and score the model. \n","\n","Since we'll be repeatedly splitting X, y into the same train/val partitions and fitting/scoring new models as we update our feature set, we'll define a reusable function that completes all these steps, making our code/process more efficient going forward. "]},{"cell_type":"markdown","metadata":{"id":"Vo8bfSH3hANB"},"source":["Great, let's go ahead and run this function on our baseline feature set and take some time to analyze the results."]},{"cell_type":"markdown","metadata":{"id":"KgGP_AklhANB"},"source":["### Basic feature engineering: adding polynomial and interaction terms"]},{"cell_type":"markdown","metadata":{"id":"Vp4bJGpahANB"},"source":["One of the first things that we looked for in the pairplot was evidence about the relationship between each feature and the target. In certain features like _'Overall Qual'_ and _'Gr Liv Qual'_, we notice an upward-curved relationship rather than a simple linear correspondence. This suggests that we should add quadratic **polynomial terms or transformations** for those features, allowing us to express that non-linear relationship while still using linear regression as our model.\n","\n","Luckily, pandas makes it quite easy to quickly add those square terms as additional features to our original feature set. We'll do so and evaluate our model again below.\n","\n","As we add to our baseline set of features, we'll create a copy of the latest benchmark so that we can continue to store our older feature sets. \n","### Polynomial Features"]},{"cell_type":"code","metadata":{"id":"lWo-WtMFhANC"},"source":["X2 = X.copy()\n","\n","X2['OQ2'] = X2['Overall Qual'] ** 2\n","X2['GLA2'] = X2['Gr Liv Area'] ** 2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cHTXwmIhANC"},"source":["As is, each feature is treated as an independent quantity. However, there may be **interaction effects**, in which the impact of one feature may dependent on the current value of a different feature.\n","\n","For example, there may be a higher premium for increasing _'Overall Qual'_ for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies _'Overall Qual'_ by _'Year Built'_ can help us capture it.\n","\n","Another style of interaction term involves feature proprtions: for example, to get at something like quality per square foot we could divide _'Overall Qual'_ by _'Lot Area'_.\n","\n","Let's try adding both of these interaction terms and see how they impact the model results.\n","\n","### Feature interactions"]},{"cell_type":"code","metadata":{"id":"JSR6_KuEhANC"},"source":["X3 = X2.copy()\n","\n","# multiplicative interaction\n","X3['OQ_x_YB'] = X3['Overall Qual'] * X3['Year Built']\n","\n","# division interaction\n","X3['OQ_/_LA'] = X3['Overall Qual'] / X3['Lot Area']\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDCzrHAOhANC"},"source":["-----\n","**Interaction Feature Exercise**: What other interactions do you think might be helpful? Why? \n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"lK3sD085hANC"},"source":["### Categories and features derived from category aggregates "]},{"cell_type":"markdown","metadata":{"id":"NafZQNGZhANC"},"source":["Incorporating **categorical features** into linear regression models is fairly straightforward: we can create a new feature column for each category value, and fill these columns with 1s and 0s to indicate which category is present for each row. This method is called **dummy variables** or **one-hot-encoding**.\n","\n","We'll first explore this using the _'House Style'_ feature from the original dataframe. Before going straight to dummy variables, it's a good idea to check category counts to make sure all categories have reasonable representation."]},{"cell_type":"code","metadata":{"id":"znviduL6hANC"},"source":["data['House Style'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5945u1DohAND"},"source":["This looks ok, and here's a quick look at how dummy features actually appear:"]},{"cell_type":"code","metadata":{"id":"tjj6v_KGhAND"},"source":["pd.get_dummies(df['House Style'], drop_first=True).head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rj9PERPphAND"},"source":["We can call `pd.get_dummies()` on our entire dataset to quickly get data with all the original features and dummy variable representation of any categorical features. Let's look at some variable values."]},{"cell_type":"code","metadata":{"id":"IL_maq5YhAND"},"source":["nbh_counts = df.Neighborhood.value_counts()\n","nbh_counts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-rsReTwhAND"},"source":["For this category, let's map the few least-represented neighborhoods to an \"other\" category before adding the feature to our feature set and running a new benchmark."]},{"cell_type":"code","metadata":{"id":"Pyqkyp78hAND"},"source":["other_nbhs = list(nbh_counts[nbh_counts <= 8].index)\n","\n","other_nbhs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bTSuxDtBhAND"},"source":["X4 = X3.copy()\n","\n","X4['Neighborhood'] = df['Neighborhood'].replace(other_nbhs, 'Other')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CV7xTv-sYoxL"},"source":["X4.Neighborhood.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCsy7Z3KhAND"},"source":["#### Getting to fancier features\n","\n","Let's close out our introduction to feature engineering by considering a more complex type of feature that may work very nicely for certain problems. It doesn't seem to add a great deal over what we have so far, but it's a style of engineering to keep in mind for the future.\n","\n","We'll create features that capture where a feature value lies relative to the members of a category it belongs to. In particular, we'll calculate deviance of a row's feature value from the mean value of the category that row belongs to. This helps to capture information about a feature relative to the category's distribution, e.g. how nice a house is relative to other houses in its neighborhood or of its style.\n","\n","Below we define reusable code for generating features of this form, feel free to repurpose it for future feature engineering work!"]},{"cell_type":"code","metadata":{"id":"mdYUvNSNhANE"},"source":["def add_deviation_feature(X, feature, category):\n","    \n","    # temp groupby object\n","    category_gb = X.groupby(category)[feature]\n","    \n","    # create category means and standard deviations for each observation\n","    category_mean = category_gb.transform(lambda x: x.mean())\n","    category_std = category_gb.transform(lambda x: x.std())\n","    \n","    # compute stds from category mean for each feature value,\n","    # add to X as new feature\n","    deviation_feature = (X[feature] - category_mean) / category_std \n","    X[feature + '_Dev_' + category] = deviation_feature  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7voSEkyHhANE"},"source":["And now let's use our feature generation code to add 2 new deviation features, and run a final benchmark."]},{"cell_type":"code","metadata":{"id":"xDZ_KADchANE"},"source":["X5 = X4.copy()\n","X5['House Style'] = df['House Style']\n","add_deviation_feature(X5, 'Year Built', 'House Style')\n","add_deviation_feature(X5, 'Overall Qual', 'Neighborhood')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-EJm0iLxhANE"},"source":["## Polynomial Features in Scikit-Learn\n","\n","`sklearn` allows you to build many higher-order terms at once with `PolynomialFeatures`"]},{"cell_type":"code","metadata":{"id":"iDDzXAW6hANE"},"source":["from sklearn.preprocessing import PolynomialFeatures"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W33WzDxJhANE"},"source":["#Instantiate and provide desired degree; \n","#   Note: degree=2 also includes intercept, degree 1 terms, and cross-terms\n","\n","pf = PolynomialFeatures(degree=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIH_CqTXhANF"},"source":["features = ['Lot Area', 'Overall Qual']\n","pf.fit(df[features])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbN2niDhhANF"},"source":["pf.get_feature_names()  #Must add input_features = features for appropriate names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3McldquhANF"},"source":["feat_array = pf.transform(df[features])\n","pd.DataFrame(feat_array, columns = pf.get_feature_names(input_features=features))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0SJniqAhANF"},"source":["## Recap\n","\n","While we haven't yet turned to prediction, these feature engineering exercises set the stage. Generally, feature engineering often follows a sort of [_Pareto principle_](https://en.wikipedia.org/wiki/Pareto_principle), where a large bulk of the predictive gains can be reached through adding a set of intuitive, strong features like polynomial transforms and interactions. Directly incorporating additional information like categorical variables can also be very helpful. Beyond this point, additional feature engineering can provide significant, but potentially diminishing returns. Whether it's worth it depends on the use case for the model. "]},{"cell_type":"markdown","metadata":{"id":"Mg8dBw1QhANF"},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation"]}]}